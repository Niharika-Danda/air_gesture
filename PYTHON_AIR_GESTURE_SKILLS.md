# Complete Python Skills Guide: Building Air Gesture Control Applications

**Comprehensive Learning Resource for Building State-of-the-Art Gesture Recognition Systems**

**Version:** 1.0
**Last Updated:** 2026-02-05
**Target Level:** Intermediate to Advanced Python Developers

---

## Table of Contents

1. [Introduction](#introduction)
2. [Core Concepts](#core-concepts)
3. [Essential Python Skills](#essential-python-skills)
4. [Computer Vision Fundamentals](#computer-vision-fundamentals)
5. [Hand Detection & Tracking](#hand-detection--tracking)
6. [Gesture Recognition Algorithms](#gesture-recognition-algorithms)
7. [Real-Time Processing Architecture](#real-time-processing-architecture)
8. [GUI Development](#gui-development)
9. [System Integration](#system-integration)
10. [Advanced Optimization Techniques](#advanced-optimization-techniques)
11. [Testing & Debugging](#testing--debugging)
12. [Deployment & Distribution](#deployment--distribution)

---

## Introduction

### What You'll Learn

This guide covers all skills needed to build a professional air gesture control application including:

- **Computer Vision:** Real-time hand detection and tracking
- **Gesture Recognition:** Static poses and dynamic swipes
- **Real-Time Processing:** Multi-threaded architecture for <100ms latency
- **Modern GUI:** CustomTkinter for professional Windows applications
- **System Integration:** Keyboard/mouse control and window management
- **Performance Optimization:** Handle 30+ FPS on standard hardware
- **Professional Deployment:** Package as standalone .exe with installer

### Prerequisites

- **Python Knowledge:** Intermediate (classes, decorators, async patterns)
- **Math Background:** Basic linear algebra and geometry
- **Development Environment:** Windows 10+ with webcam
- **Tools:** VS Code, Git, pip

### Application Stack

```
User Input (Camera Feed)
    ‚Üì
Computer Vision Layer (OpenCV + MediaPipe)
    ‚Üì
Gesture Recognition Engine
    ‚Üì
Event Processing (Event Bus)
    ‚Üì
System Control (pyautogui + Win32)
    ‚Üì
User Feedback (Audio + Visual)
```

---

## Core Concepts

### 1. Real-Time Processing Pipeline

#### The Gesture Recognition Flow

```python
"""
Core Pipeline Flow:
1. Capture frame from webcam (30-60 FPS)
2. Detect hand landmarks (21 points per hand)
3. Apply temporal smoothing (filter jitter)
4. Classify gesture (ML model or geometric rules)
5. Execute action (keyboard/mouse)
6. Provide feedback (audio/visual)
"""

# Pseudo-code of the pipeline
def process_frame(frame):
    """Main processing pipeline"""
    # Step 1: Detect hands
    hands = mediapipe.detect_hands(frame)

    # Step 2: Get landmarks (21 points per hand)
    landmarks = hands.hand_landmarks

    # Step 3: Smooth landmarks to reduce noise
    smoothed = temporal_smoother.apply(landmarks)

    # Step 4: Recognize gesture
    gesture = gesture_classifier.recognize(smoothed)

    # Step 5: Execute action
    if gesture in shortcut_map:
        execute_action(shortcut_map[gesture])

    # Step 6: Feedback
    play_sound("gesture_confirmed.wav")

    return gesture
```

### 2. Latency Requirements

Professional gesture control requires sub-100ms latency:

```
Target Latency Budget:
‚îú‚îÄ‚îÄ Camera Capture: 16ms (60 FPS)
‚îú‚îÄ‚îÄ Detection: 25ms (MediaPipe)
‚îú‚îÄ‚îÄ Processing: 10ms (smoothing, classification)
‚îú‚îÄ‚îÄ Execution: 5ms (keyboard event)
‚îú‚îÄ‚îÄ Feedback: 10ms (audio)
‚îî‚îÄ‚îÄ Total: ~66ms (excellent)

Real-World (30 FPS camera):
‚îú‚îÄ‚îÄ Capture: 33ms
‚îú‚îÄ‚îÄ Detection: 35ms
‚îú‚îÄ‚îÄ Processing: 15ms
‚îú‚îÄ‚îÄ Total: ~83ms (acceptable)
```

### 3. Thread Safety & Concurrency

Gesture apps require parallel processing:

```python
"""
Threading Model:
- Main Thread: Tkinter event loop + UI updates
- Vision Thread: Camera capture + gesture detection
- Action Thread: Keyboard/mouse execution (if needed)

Communication: Thread-safe queues and event bus
"""

from queue import Queue
from threading import Thread, Lock

class ThreadSafeGestureApp:
    def __init__(self):
        self.frame_queue = Queue(maxsize=1)  # Latest frame
        self.gesture_queue = Queue()          # Detected gestures
        self.lock = Lock()                    # Protect shared state

    def vision_thread(self):
        """Runs on dedicated thread"""
        while not self.stop_event.is_set():
            frame = self.camera.read()
            gesture = self.detector.recognize(frame)
            self.gesture_queue.put(gesture)

    def main_thread(self):
        """UI and event handling"""
        while True:
            try:
                gesture = self.gesture_queue.get(timeout=0.1)
                self.handle_gesture(gesture)
            except:
                pass
```

---

## Essential Python Skills

### 1. Object-Oriented Design for Gesture Apps

#### Design Pattern: Component Architecture

```python
"""
Skill: Building modular, testable components
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, List

# Define data structures
@dataclass
class HandLandmarks:
    """Immutable hand landmark data"""
    x: List[float]
    y: List[float]
    z: List[float]
    confidence: List[float]

    def get_thumb_tip(self) -> tuple:
        """Get 3D position of thumb tip"""
        return (self.x[4], self.y[4], self.z[4])

# Abstract base classes for extensibility
class GestureDetector(ABC):
    """Interface for any gesture detector"""

    @abstractmethod
    def detect(self, landmarks: HandLandmarks) -> Optional[str]:
        """Detect gesture from landmarks"""
        pass

class StaticGestureDetector(GestureDetector):
    """Detect static poses (thumbs up, OK sign, etc.)"""

    def detect(self, landmarks: HandLandmarks) -> Optional[str]:
        """Recognize pose-based gestures"""

        # Calculate geometric properties
        thumb_tip = landmarks.get_thumb_tip()
        index_tip = self.get_landmark(landmarks, 8)  # Index finger tip

        # Detect "thumbs up"
        if self._is_thumbs_up(landmarks):
            return "THUMBS_UP"

        # Detect "OK sign"
        if self._is_ok_sign(landmarks):
            return "OK_SIGN"

        return None

    def _is_thumbs_up(self, landmarks: HandLandmarks) -> bool:
        """Check if hand forms thumbs up gesture"""
        # Thumb should be pointing up
        # Other fingers should be curled
        thumb_y = landmarks.y[4]
        index_y = landmarks.y[8]
        middle_y = landmarks.y[12]

        # Thumb tip higher than finger tips
        return thumb_y < index_y and thumb_y < middle_y

class DynamicGestureDetector(GestureDetector):
    """Detect movement-based gestures (swipes)"""

    def __init__(self, buffer_size: int = 10):
        self.landmark_history = []
        self.buffer_size = buffer_size

    def detect(self, landmarks: HandLandmarks) -> Optional[str]:
        """Recognize swipe gestures from trajectory"""

        # Add to history
        self.landmark_history.append(landmarks)
        if len(self.landmark_history) > self.buffer_size:
            self.landmark_history.pop(0)

        # Need minimum history
        if len(self.landmark_history) < 3:
            return None

        # Analyze trajectory
        velocity = self._calculate_velocity()
        direction = self._calculate_direction(velocity)

        if self._is_swipe(velocity, direction):
            return f"SWIPE_{direction}"

        return None

    def _calculate_velocity(self) -> float:
        """Calculate hand movement speed"""
        if len(self.landmark_history) < 2:
            return 0

        current = self.landmark_history[-1]
        previous = self.landmark_history[-2]

        # Euclidean distance between palm centers
        dx = current.x[9] - previous.x[9]  # Palm center X
        dy = current.y[9] - previous.y[9]  # Palm center Y

        return (dx**2 + dy**2)**0.5  # Distance

# Dependency injection for testability
class GestureEngine:
    """Main gesture recognition engine"""

    def __init__(self,
                 static_detector: StaticGestureDetector,
                 dynamic_detector: DynamicGestureDetector):
        self.static = static_detector
        self.dynamic = dynamic_detector

    def recognize(self, landmarks: HandLandmarks) -> Optional[str]:
        """Try both detection methods"""
        # Static first (faster)
        gesture = self.static.detect(landmarks)
        if gesture:
            return gesture

        # Then dynamic
        return self.dynamic.detect(landmarks)

# Usage
detector = GestureEngine(
    static_detector=StaticGestureDetector(),
    dynamic_detector=DynamicGestureDetector()
)
```

### 2. Advanced Type Hinting for Safety

```python
"""
Skill: Using type hints for robust code
"""

from typing import Tuple, Dict, List, Optional, Union, Callable
from enum import Enum

class GestureType(Enum):
    """Enumerated gesture types"""
    THUMBS_UP = "THUMBS_UP"
    SWIPE_LEFT = "SWIPE_LEFT"
    SWIPE_RIGHT = "SWIPE_RIGHT"
    OK_SIGN = "OK_SIGN"

class Point3D(tuple):
    """3D coordinate point"""
    def __new__(cls, x: float, y: float, z: float):
        return super().__new__(cls, (x, y, z))

    @property
    def x(self) -> float:
        return self[0]

    @property
    def magnitude(self) -> float:
        return (self.x**2 + self[1]**2 + self[2]**2)**0.5

# Type aliases for clarity
Landmarks = List[Point3D]  # 21 points per hand
GestureCallback = Callable[[GestureType, float], None]  # (gesture, confidence)

def detect_gesture(landmarks: Landmarks,
                  confidence_threshold: float = 0.7) -> Optional[GestureType]:
    """Type-safe gesture detection"""
    pass

def register_gesture_handler(gesture: GestureType,
                            handler: GestureCallback) -> None:
    """Register callback for specific gesture"""
    pass
```

### 3. Decorator Pattern for Cross-Cutting Concerns

```python
"""
Skill: Using decorators for timing, logging, caching
"""

import time
import functools
from typing import Any, Callable

def measure_performance(func: Callable) -> Callable:
    """Decorator to measure function execution time"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter() - start) * 1000  # ms

        if elapsed > 50:  # Log slow operations
            print(f"‚ö†Ô∏è  {func.__name__} took {elapsed:.1f}ms")

        return result
    return wrapper

def cache_gesture(ttl_seconds: float = 0.5) -> Callable:
    """Decorator to cache gesture recognition result"""
    def decorator(func: Callable) -> Callable:
        cache = {}
        last_clear = time.time()

        @functools.wraps(func)
        def wrapper(landmarks: Landmarks) -> Optional[str]:
            nonlocal last_clear

            # Clear cache after TTL
            if time.time() - last_clear > ttl_seconds:
                cache.clear()
                last_clear = time.time()

            # Use tuple as key
            key = tuple(tuple(p) for p in landmarks)
            if key not in cache:
                cache[key] = func(landmarks)

            return cache[key]

        return wrapper
    return decorator

@cache_gesture(ttl_seconds=0.1)
@measure_performance
def recognize_gesture(landmarks: Landmarks) -> Optional[str]:
    """Optimized gesture recognition with caching"""
    # Expensive computation here
    pass
```

### 4. Context Managers for Resource Management

```python
"""
Skill: Proper resource cleanup with context managers
"""

from contextlib import contextmanager
import threading

class CameraManager:
    """Manages camera lifecycle"""

    def __init__(self, camera_index: int = 0):
        self.camera_index = camera_index
        self.camera = None

    def __enter__(self):
        """Open camera when entering context"""
        self.camera = cv2.VideoCapture(self.camera_index)
        if not self.camera.isOpened():
            raise RuntimeError(f"Camera {self.camera_index} not found")
        return self.camera

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Close camera when exiting context"""
        if self.camera:
            self.camera.release()
        return False

# Usage
with CameraManager(camera_index=0) as camera:
    ret, frame = camera.read()
    # Camera automatically closed when exiting block

@contextmanager
def gesture_timeout(seconds: float):
    """Context manager for gesture recognition timeout"""
    start = time.time()

    try:
        yield
    finally:
        elapsed = time.time() - start
        if elapsed > seconds:
            print(f"‚ö†Ô∏è  Gesture recognition exceeded {seconds}s limit")
```

### 5. Custom Exceptions for Error Handling

```python
"""
Skill: Designing custom exceptions for better error handling
"""

class GestureException(Exception):
    """Base exception for gesture detection"""
    pass

class CameraException(GestureException):
    """Camera-related errors"""
    pass

class HandDetectionException(GestureException):
    """Hand detection failures"""
    pass

class GestureClassificationException(GestureException):
    """Gesture recognition failures"""
    pass

class ConfigurationException(GestureException):
    """Configuration validation errors"""
    pass

# Usage
class GestureApp:
    def run(self):
        try:
            camera = self._initialize_camera()
        except CameraException as e:
            print(f"‚ùå Camera Error: {e}")
            print("Please check camera connection")
            return False
        except ConfigurationException as e:
            print(f"‚ùå Config Error: {e}")
            return False
        except GestureException as e:
            print(f"‚ùå Unexpected Error: {e}")
            return False
```

---

## Computer Vision Fundamentals

### 1. Image Processing Basics

#### Understanding Image Data

```python
"""
Skill: Working with image data and pixel operations
"""

import cv2
import numpy as np

def understand_image_format():
    """Learn image data structure"""

    # Load image (BGR format in OpenCV)
    img = cv2.imread('frame.jpg')
    print(f"Shape: {img.shape}")  # (480, 640, 3) = height √ó width √ó channels

    # Channels: Blue, Green, Red (BGR, not RGB!)
    height, width, channels = img.shape

    # Access pixel at (100, 200)
    bgr = img[100, 200]  # [Blue, Green, Red]
    b, g, r = bgr

    # Modify pixel
    img[100, 200] = [255, 0, 0]  # Pure blue

    return img

def color_space_conversion():
    """Convert between color spaces"""

    img = cv2.imread('frame.jpg')

    # BGR to RGB (correct display order)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # BGR to Grayscale (single channel, faster processing)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # BGR to HSV (better for color detection)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # HSV format: Hue [0-179], Saturation [0-255], Value [0-255]
    # Better for detecting specific colors (e.g., skin tone)

    return img_rgb, gray, hsv

def image_filtering():
    """Apply filters for noise reduction"""

    img = cv2.imread('frame.jpg')

    # Gaussian Blur: Smooth image, reduce noise
    blurred = cv2.GaussianBlur(img, (5, 5), 0)

    # Bilateral Filter: Blur while preserving edges
    bilateral = cv2.bilateralFilter(img, 9, 75, 75)

    # Morphological operations: Open/close shapes
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    opened = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)

    return blurred, bilateral, opened

def edge_detection():
    """Find edges in image"""

    img = cv2.imread('frame.jpg')
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Canny edge detection
    edges = cv2.Canny(gray, 100, 200)

    # Sobel (calculate gradients)
    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)

    return edges, sobelx, sobely

# Real-world: Prepare camera frame for processing
def preprocess_frame(frame, target_height=480):
    """Optimize frame for gesture detection"""

    height, width = frame.shape[:2]

    # Resize to target resolution (faster processing)
    scale = target_height / height
    new_width = int(width * scale)
    frame = cv2.resize(frame, (new_width, target_height))

    # Convert to RGB for MediaPipe
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Optional: Enhance contrast for better detection
    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    l = clahe.apply(l)
    enhanced = cv2.merge([l, a, b])
    frame_rgb = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)

    return frame_rgb
```

### 2. Object Detection & Contour Analysis

```python
"""
Skill: Detecting objects and analyzing shapes in images
"""

import cv2
import numpy as np

def detect_hand_shape(frame):
    """Traditional approach: detect hand using contours"""

    # Skin color detection in HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    # Define skin color range (varies by ethnicity, lighting)
    lower_skin = np.array([0, 20, 70], dtype=np.uint8)
    upper_skin = np.array([20, 255, 255], dtype=np.uint8)

    # Create binary mask
    mask = cv2.inRange(hsv, lower_skin, upper_skin)

    # Clean up mask
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

    # Find contours
    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        return None

    # Get largest contour (most likely the hand)
    largest = max(contours, key=cv2.contourArea)
    area = cv2.contourArea(largest)

    if area < 500:  # Too small to be a hand
        return None

    # Get contour properties
    x, y, w, h = cv2.boundingRect(largest)
    hull = cv2.convexHull(largest)

    # Count fingers (convexity defects)
    defects = cv2.convexityDefects(largest, hull)

    if defects is not None:
        finger_count = 0
        for i in range(defects.shape[0]):
            s, e, f, d = defects[i, 0]
            if d > 50:  # Depth threshold
                finger_count += 1

    return {
        'contour': largest,
        'bbox': (x, y, w, h),
        'hull': hull,
        'fingers': finger_count + 1
    }

def match_template(frame, template):
    """Find template image in frame"""

    result = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

    if max_val > 0.8:  # Good match threshold
        return {
            'location': max_loc,
            'confidence': max_val,
            'size': template.shape[:2]
        }

    return None

def perspective_transformation():
    """Transform camera view to bird's eye view"""

    frame = cv2.imread('frame.jpg')
    height, width = frame.shape[:2]

    # Define original points (camera view)
    src_points = np.float32([
        [0, 0],
        [width, 0],
        [0, height],
        [width, height]
    ])

    # Define target points (bird's eye)
    dst_points = np.float32([
        [width * 0.2, 0],
        [width * 0.8, 0],
        [0, height],
        [width, height]
    ])

    # Get transformation matrix
    matrix = cv2.getPerspectiveTransform(src_points, dst_points)

    # Apply transformation
    warped = cv2.warpPerspective(frame, matrix, (width, height))

    return warped
```

### 3. Camera Calibration & Undistortion

```python
"""
Skill: Improving camera accuracy with calibration
"""

import cv2
import numpy as np

def camera_calibration():
    """Calibrate camera to remove lens distortion"""

    # Chessboard pattern used for calibration
    # Print 9x6 chessboard and capture 20+ images from different angles

    # Setup
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    objp = np.zeros((6*9, 3), np.float32)
    objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)

    objpoints = []  # 3D points in real world
    imgpoints = []  # 2D points in image

    # Capture calibration images
    cap = cv2.VideoCapture(0)
    captured = 0

    while captured < 20:
        ret, frame = cap.read()
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Find chessboard corners
        ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)

        if ret:
            objpoints.append(objp)

            # Refine corner positions
            corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
            imgpoints.append(corners2)

            # Draw for visualization
            frame = cv2.drawChessboardCorners(frame, (9, 6), corners2, ret)
            cv2.imshow('Calibration', frame)

            if cv2.waitKey(100) & 0xFF == ord('s'):
                captured += 1
                print(f"Captured {captured}/20")

    cap.release()
    cv2.destroyAllWindows()

    # Calibrate camera
    ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
        objpoints, imgpoints, gray.shape[::-1], None, None
    )

    return camera_matrix, dist_coeffs

def apply_calibration(frame, camera_matrix, dist_coeffs):
    """Remove lens distortion from frame"""

    # Undistort image
    undistorted = cv2.undistort(frame, camera_matrix, dist_coeffs)

    return undistorted

# One-time setup
# camera_matrix, dist_coeffs = camera_calibration()
# Save for later: np.save('camera_matrix.npy', camera_matrix)
```

---

## Hand Detection & Tracking

### 1. MediaPipe Hand Detection

#### Understanding MediaPipe Hands

```python
"""
Skill: Using MediaPipe for accurate hand detection and tracking
MediaPipe detects 21 hand landmarks per hand in real-time
"""

import mediapipe as mp
import cv2
import numpy as np

class HandDetector:
    """Wrapper around MediaPipe for hand detection"""

    def __init__(self, max_hands: int = 2, confidence: float = 0.5):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=max_hands,
            min_detection_confidence=confidence,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils

    def detect(self, frame: np.ndarray) -> dict:
        """
        Detect hands in frame

        Returns:
            {
                'hands': [
                    {
                        'landmarks': [(x, y, z), ...],  # 21 points
                        'handedness': 'Left'/'Right',
                        'confidence': 0.95
                    }
                ],
                'frame_width': int,
                'frame_height': int
            }
        """

        height, width, _ = frame.shape

        # Convert BGR to RGB for MediaPipe
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(frame_rgb)

        detected_hands = []

        if results.multi_hand_landmarks:
            for hand_landmarks, handedness in zip(
                results.multi_hand_landmarks,
                results.multi_handedness
            ):
                # Convert to normalized coordinates
                landmarks = []
                for lm in hand_landmarks.landmark:
                    landmarks.append((lm.x, lm.y, lm.z))

                detected_hands.append({
                    'landmarks': landmarks,
                    'handedness': handedness.classification[0].label,
                    'confidence': handedness.classification[0].score
                })

        return {
            'hands': detected_hands,
            'frame_width': width,
            'frame_height': height
        }

    def draw_landmarks(self, frame: np.ndarray, results: dict) -> np.ndarray:
        """Draw hand landmarks on frame"""

        if not results['hands']:
            return frame

        # Simple drawing
        for hand in results['hands']:
            landmarks = hand['landmarks']

            # Draw circles at landmark points
            for i, (x, y, z) in enumerate(landmarks):
                px = int(x * results['frame_width'])
                py = int(y * results['frame_height'])
                cv2.circle(frame, (px, py), 3, (0, 255, 0), -1)

            # Draw connections between landmarks
            connections = [
                (0, 1), (1, 2), (2, 3), (3, 4),      # Thumb
                (0, 5), (5, 6), (6, 7), (7, 8),      # Index
                (0, 9), (9, 10), (10, 11), (11, 12), # Middle
                (0, 13), (13, 14), (14, 15), (15, 16), # Ring
                (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky
            ]

            for start, end in connections:
                x1, y1, _ = landmarks[start]
                x2, y2, _ = landmarks[end]

                px1, py1 = int(x1 * results['frame_width']), int(y1 * results['frame_height'])
                px2, py2 = int(x2 * results['frame_width']), int(y2 * results['frame_height'])

                cv2.line(frame, (px1, py1), (px2, py2), (0, 255, 0), 2)

        return frame

# Usage
def main_detection_loop():
    detector = HandDetector(max_hands=2, confidence=0.7)
    cap = cv2.VideoCapture(0)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Detect hands
        results = detector.detect(frame)

        # Draw results
        frame = detector.draw_landmarks(frame, results)

        # Process detected hands
        for hand in results['hands']:
            print(f"Hand: {hand['handedness']}, Confidence: {hand['confidence']:.2f}")
            print(f"Landmarks: {len(hand['landmarks'])} points")

        cv2.imshow('Hand Detection', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
```

### 2. Hand Landmark Analysis

#### Understanding the 21 Landmarks

```python
"""
Skill: Understanding and using MediaPipe hand landmarks

Hand Landmarks (21 points):
0: Wrist (base of hand)
1-4: Thumb
5-8: Index finger
9-12: Middle finger
13-16: Ring finger
17-20: Pinky finger

Each landmark has: x (0-1), y (0-1), z (depth)
"""

from dataclasses import dataclass
from typing import List, Tuple

@dataclass
class HandGeometry:
    """Calculate hand geometry properties"""

    landmarks: List[Tuple[float, float, float]]  # 21 points

    def get_landmark(self, index: int) -> Tuple[float, float, float]:
        """Get a specific landmark"""
        return self.landmarks[index]

    def distance(self, idx1: int, idx2: int) -> float:
        """Calculate distance between two landmarks"""
        x1, y1, z1 = self.landmarks[idx1]
        x2, y2, z2 = self.landmarks[idx2]
        return ((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)**0.5

    def angle(self, idx1: int, idx2: int, idx3: int) -> float:
        """Calculate angle between three landmarks (degrees)"""

        # Get vectors
        p1 = np.array(self.landmarks[idx1])
        p2 = np.array(self.landmarks[idx2])
        p3 = np.array(self.landmarks[idx3])

        v1 = p1 - p2
        v2 = p3 - p2

        # Calculate angle using dot product
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
        angle = np.arccos(np.clip(cos_angle, -1, 1))

        return np.degrees(angle)

    def is_finger_extended(self, finger_tip_idx: int, pip_idx: int) -> bool:
        """Check if finger is extended"""
        # Tip should be higher (lower Y) than PIP joint
        tip_y = self.landmarks[finger_tip_idx][1]
        pip_y = self.landmarks[pip_idx][1]
        return tip_y < pip_y

    def get_hand_center(self) -> Tuple[float, float]:
        """Calculate center of hand (palm)"""
        x_coords = [lm[0] for lm in self.landmarks]
        y_coords = [lm[1] for lm in self.landmarks]
        return (np.mean(x_coords), np.mean(y_coords))

    def get_hand_orientation(self) -> str:
        """Determine if hand is facing up, down, left, right"""

        # Use palm normal (z-axis)
        # More negative Z = hand is facing away
        palm_positions = self.landmarks[0:5]  # Wrist + finger bases
        avg_z = np.mean([p[2] for p in palm_positions])

        if avg_z > 0.1:
            return "FACING_CAMERA"
        elif avg_z < -0.1:
            return "FACING_AWAY"
        else:
            return "SIDE_VIEW"

    def get_hand_size(self) -> float:
        """Estimate hand size"""
        # Distance from wrist to middle finger tip
        wrist = np.array(self.landmarks[0])
        middle_tip = np.array(self.landmarks[12])
        return np.linalg.norm(middle_tip - wrist)

# Usage: Detect specific hand poses
class HandPoseClassifier:
    """Classify hand poses based on geometry"""

    def __init__(self):
        self.landmark_history = []
        self.history_size = 5

    def classify(self, landmarks: List) -> str:
        """Classify hand pose"""

        geometry = HandGeometry(landmarks)

        # Check for OK sign (thumb + index touching)
        thumb_tip = geometry.landmarks[4]
        index_tip = geometry.landmarks[8]
        distance = geometry.distance(4, 8)

        if distance < 0.05:  # Thumb and index close
            # Check other fingers extended
            extended_fingers = sum([
                geometry.is_finger_extended(12, 10),  # Middle
                geometry.is_finger_extended(16, 14),  # Ring
                geometry.is_finger_extended(20, 18)   # Pinky
            ])

            if extended_fingers >= 2:
                return "OK_SIGN"

        # Check for thumbs up
        if geometry.angle(2, 3, 4) > 160:  # Thumb nearly straight
            thumb_y = geometry.landmarks[4][1]
            palm_y = geometry.landmarks[0][1]

            if thumb_y < palm_y:  # Thumb pointing up
                return "THUMBS_UP"

        # Check for peace sign
        if (geometry.is_finger_extended(8, 6) and  # Index extended
            geometry.is_finger_extended(12, 10) and  # Middle extended
            not geometry.is_finger_extended(16, 14)):  # Ring folded
            return "PEACE_SIGN"

        # Check for open palm
        if all([geometry.is_finger_extended(i, i-2) for i in [8, 12, 16, 20]]):
            return "OPEN_PALM"

        # Check for point gesture
        if (geometry.is_finger_extended(8, 6) and  # Index extended
            not geometry.is_finger_extended(12, 10)):  # Others folded
            return "POINT"

        return "UNKNOWN"
```

### 3. Multi-Hand Tracking

```python
"""
Skill: Track multiple hands across frames to maintain identity
"""

from dataclasses import dataclass
from typing import List, Optional
import uuid

@dataclass
class Hand:
    """Represents a tracked hand"""
    hand_id: str
    handedness: str  # 'Left' or 'Right'
    landmarks: List[Tuple[float, float, float]]
    confidence: float
    age_frames: int = 0  # How long detected

    def distance_to(self, other_landmarks: List) -> float:
        """Calculate distance to other hand"""
        current_center = np.mean(self.landmarks, axis=0)
        other_center = np.mean(other_landmarks, axis=0)
        return np.linalg.norm(current_center - other_center)

class MultiHandTracker:
    """Track multiple hands across frames"""

    def __init__(self, max_hands: int = 2):
        self.tracked_hands: List[Hand] = []
        self.max_hands = max_hands
        self.distance_threshold = 0.1  # Max distance to match

    def update(self, detected_hands: List[dict]) -> List[Hand]:
        """Update hand tracking with new detections"""

        updated_hands = []

        # If no hands detected
        if not detected_hands:
            # Age tracked hands
            for hand in self.tracked_hands:
                hand.age_frames += 1
            # Remove hands not detected for >30 frames
            self.tracked_hands = [h for h in self.tracked_hands if h.age_frames < 30]
            return self.tracked_hands

        # Match detected hands to tracked hands
        matched_detected = set()

        for tracked in self.tracked_hands:
            best_match = None
            best_distance = self.distance_threshold
            best_idx = -1

            for i, detected in enumerate(detected_hands):
                if i in matched_detected:
                    continue

                # Check if handedness matches
                if detected['handedness'] == tracked.handedness:
                    dist = tracked.distance_to(detected['landmarks'])

                    if dist < best_distance:
                        best_distance = dist
                        best_match = detected
                        best_idx = i

            if best_match:
                # Update tracked hand
                tracked.landmarks = best_match['landmarks']
                tracked.confidence = best_match['confidence']
                tracked.age_frames = 0
                updated_hands.append(tracked)
                matched_detected.add(best_idx)

        # Create new tracks for unmatched detections
        for i, detected in enumerate(detected_hands):
            if i not in matched_detected:
                new_hand = Hand(
                    hand_id=str(uuid.uuid4()),
                    handedness=detected['handedness'],
                    landmarks=detected['landmarks'],
                    confidence=detected['confidence']
                )
                updated_hands.append(new_hand)

        self.tracked_hands = updated_hands
        return updated_hands
```

---

## Gesture Recognition Algorithms

### 1. Static Gesture Recognition

#### Geometric-Based Approach

```python
"""
Skill: Recognize static hand poses using geometric rules

Key Idea: Analyze hand geometry to identify poses
"""

class StaticGestureRecognizer:
    """Recognize static hand poses"""

    # Define confidence thresholds
    EXTENDED_THRESHOLD = 0.7  # How extended a finger should be
    TOUCHING_THRESHOLD = 0.05  # How close fingers can be
    ANGLE_TOLERANCE = 20  # Degrees

    def __init__(self):
        self.geometry = None

    def recognize(self, landmarks: List) -> str:
        """Main recognition method"""

        self.geometry = HandGeometry(landmarks)

        # Try to match known gestures
        if self._is_thumbs_up():
            return "THUMBS_UP"
        elif self._is_thumbs_down():
            return "THUMBS_DOWN"
        elif self._is_ok_sign():
            return "OK_SIGN"
        elif self._is_open_palm():
            return "OPEN_PALM"
        elif self._is_peace_sign():
            return "PEACE_SIGN"
        elif self._is_rock_gesture():
            return "ROCK"
        elif self._is_pointing():
            return "POINT"

        return None

    def _is_thumbs_up(self) -> bool:
        """Detect thumbs up gesture

        Characteristics:
        - Thumb pointing straight up
        - Other fingers folded
        - Thumb tip above palm
        """

        g = self.geometry

        # Thumb should be nearly straight (angle > 160¬∞)
        thumb_angle = g.angle(1, 2, 4)
        if thumb_angle < 160:
            return False

        # Thumb tip should be above wrist (lower Y)
        thumb_tip_y = g.landmarks[4][1]
        wrist_y = g.landmarks[0][1]
        if thumb_tip_y >= wrist_y:
            return False

        # Other fingers should be folded
        fingers_folded = sum([
            not g.is_finger_extended(8, 6),   # Index
            not g.is_finger_extended(12, 10), # Middle
            not g.is_finger_extended(16, 14), # Ring
            not g.is_finger_extended(20, 18)  # Pinky
        ])

        return fingers_folded >= 3

    def _is_thumbs_down(self) -> bool:
        """Detect thumbs down gesture"""

        g = self.geometry

        # Thumb nearly straight
        thumb_angle = g.angle(1, 2, 4)
        if thumb_angle < 160:
            return False

        # Thumb tip below wrist
        thumb_tip_y = g.landmarks[4][1]
        wrist_y = g.landmarks[0][1]
        if thumb_tip_y <= wrist_y:
            return False

        # Other fingers folded
        fingers_folded = sum([
            not g.is_finger_extended(8, 6),
            not g.is_finger_extended(12, 10),
            not g.is_finger_extended(16, 14),
            not g.is_finger_extended(20, 18)
        ])

        return fingers_folded >= 3

    def _is_ok_sign(self) -> bool:
        """Detect OK sign (thumb + index touching)"""

        g = self.geometry

        # Thumb and index tips very close
        distance = g.distance(4, 8)
        if distance > self.TOUCHING_THRESHOLD:
            return False

        # Other fingers extended
        other_extended = sum([
            g.is_finger_extended(12, 10),
            g.is_finger_extended(16, 14),
            g.is_finger_extended(20, 18)
        ])

        return other_extended >= 2

    def _is_open_palm(self) -> bool:
        """Detect open palm (all fingers extended)"""

        g = self.geometry

        # All fingers extended
        all_extended = all([
            g.is_finger_extended(4, 2),   # Thumb
            g.is_finger_extended(8, 6),   # Index
            g.is_finger_extended(12, 10), # Middle
            g.is_finger_extended(16, 14), # Ring
            g.is_finger_extended(20, 18)  # Pinky
        ])

        if not all_extended:
            return False

        # Fingers should be spread (not touching)
        tips_spread = (g.distance(4, 8) > 0.08 and
                      g.distance(8, 12) > 0.08 and
                      g.distance(12, 16) > 0.08)

        return tips_spread

    def _is_peace_sign(self) -> bool:
        """Detect peace sign (index + middle extended, others folded)"""

        g = self.geometry

        # Index and middle extended
        if not (g.is_finger_extended(8, 6) and g.is_finger_extended(12, 10)):
            return False

        # Ring and pinky folded
        if g.is_finger_extended(16, 14) or g.is_finger_extended(20, 18):
            return False

        # Index and middle should be separated
        return g.distance(8, 12) > 0.05

    def _is_rock_gesture(self) -> bool:
        """Detect rock gesture (index + pinky extended)"""

        g = self.geometry

        # Index and pinky extended
        if not (g.is_finger_extended(8, 6) and g.is_finger_extended(20, 18)):
            return False

        # Middle and ring folded
        if g.is_finger_extended(12, 10) or g.is_finger_extended(16, 14):
            return False

        return True

    def _is_pointing(self) -> bool:
        """Detect pointing gesture (only index extended)"""

        g = self.geometry

        # Index extended
        if not g.is_finger_extended(8, 6):
            return False

        # Other fingers folded
        others_folded = sum([
            not g.is_finger_extended(4, 2),
            not g.is_finger_extended(12, 10),
            not g.is_finger_extended(16, 14),
            not g.is_finger_extended(20, 18)
        ])

        return others_folded >= 4
```

### 2. Dynamic Gesture Recognition (Swipes)

```python
"""
Skill: Recognize motion-based gestures using trajectory analysis

Key Idea: Track hand movement over time and analyze direction/speed
"""

import numpy as np
from collections import deque

class DynamicGestureRecognizer:
    """Recognize swipe and motion gestures"""

    def __init__(self, buffer_size: int = 10):
        self.landmark_history = deque(maxlen=buffer_size)
        self.buffer_size = buffer_size
        self.min_velocity = 0.02  # Minimum speed to detect swipe
        self.min_distance = 0.1   # Minimum distance for swipe

    def add_frame(self, landmarks: List) -> Optional[str]:
        """Process new frame and detect gestures"""

        # Add to history
        palm_center = np.mean(landmarks, axis=0)
        self.landmark_history.append(palm_center)

        # Need enough history
        if len(self.landmark_history) < 5:
            return None

        # Analyze trajectory
        gesture = self._analyze_trajectory()

        return gesture

    def _analyze_trajectory(self) -> Optional[str]:
        """Analyze hand trajectory for swipe gestures"""

        history = np.array(list(self.landmark_history))

        # Calculate displacements
        displacements = np.diff(history, axis=0)  # Change between frames

        # Total displacement
        total_displacement = history[-1] - history[0]
        total_distance = np.linalg.norm(total_displacement)

        # Check minimum distance
        if total_distance < self.min_distance:
            return None

        # Analyze direction
        dx, dy = total_displacement[0], total_displacement[1]

        # Fit line to trajectory
        x = np.arange(len(history))
        z = np.polyfit(x, history[:, 0], 1)  # Fit to x-coordinates

        slope = z[0]  # dx/frame ratio

        # Classify based on direction
        if abs(dx) > abs(dy):  # Horizontal motion
            if dx < -self.min_distance:
                return "SWIPE_LEFT"
            elif dx > self.min_distance:
                return "SWIPE_RIGHT"
        else:  # Vertical motion
            if dy < -self.min_distance:
                return "SWIPE_UP"
            elif dy > self.min_distance:
                return "SWIPE_DOWN"

        return None

    def get_velocity(self) -> float:
        """Calculate current hand velocity"""

        if len(self.landmark_history) < 2:
            return 0

        current = self.landmark_history[-1]
        previous = self.landmark_history[-2]

        distance = np.linalg.norm(current - previous)
        velocity = distance  # Per frame

        return velocity

    def clear_history(self):
        """Reset gesture history"""
        self.landmark_history.clear()

# Usage with temporal voting
class GestureClassifier:
    """Combine static and dynamic recognition with voting"""

    def __init__(self):
        self.static = StaticGestureRecognizer()
        self.dynamic = DynamicGestureRecognizer()
        self.gesture_history = deque(maxlen=3)
        self.confidence_threshold = 0.6

    def classify(self, landmarks: List) -> Optional[str]:
        """Classify gesture with temporal voting"""

        # Try static recognition
        static_gesture = self.static.recognize(landmarks)
        if static_gesture:
            self.gesture_history.append(static_gesture)

        # Try dynamic recognition
        dynamic_gesture = self.dynamic.add_frame(landmarks)
        if dynamic_gesture:
            self.gesture_history.append(dynamic_gesture)
            self.dynamic.clear_history()

        # Vote on most common gesture in history
        if not self.gesture_history:
            return None

        from collections import Counter
        counts = Counter(self.gesture_history)
        most_common = counts.most_common(1)[0]

        gesture, count = most_common
        confidence = count / len(self.gesture_history)

        if confidence > self.confidence_threshold:
            return gesture

        return None
```

### 3. Machine Learning Approach (Optional Advanced)

```python
"""
Skill: Using ML models for more robust gesture recognition

Advantages:
- Better generalization to different users
- Handles edge cases better
- Learns from training data

Disadvantage:
- Requires training data
- Slower inference than rule-based
"""

import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import pickle

class GestureMLModel:
    """Neural network for gesture classification"""

    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.gestures = ['SWIPE_LEFT', 'SWIPE_RIGHT', 'THUMBS_UP', 'OK_SIGN', 'PEACE']

    def build_model(self, input_dim: int = 63):  # 21 landmarks √ó 3 coords
        """Build neural network"""

        self.model = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='relu', input_dim=input_dim),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(len(self.gestures), activation='softmax')
        ])

        self.model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

    def preprocess(self, landmarks: List) -> np.ndarray:
        """Flatten landmarks to feature vector"""

        features = np.array(landmarks).flatten()  # Shape: (63,)
        features = self.scaler.transform([features])

        return features

    def predict(self, landmarks: List) -> tuple:
        """Predict gesture and confidence"""

        features = self.preprocess(landmarks)
        predictions = self.model.predict(features, verbose=0)[0]

        gesture_idx = np.argmax(predictions)
        confidence = float(predictions[gesture_idx])

        gesture = self.gestures[gesture_idx]

        return gesture, confidence

    def train(self, training_data: List[tuple], epochs: int = 20):
        """Train on collected gesture data

        training_data: [(landmarks, gesture_name), ...]
        """

        X = []
        y = []

        for landmarks, gesture_name in training_data:
            X.append(np.array(landmarks).flatten())
            y.append(self.gestures.index(gesture_name))

        X = np.array(X)
        y = np.array(y)

        # Scale features
        self.scaler.fit(X)
        X = self.scaler.transform(X)

        # Train model
        self.model.fit(X, y, epochs=epochs, batch_size=16, validation_split=0.2)

    def save(self, path: str):
        """Save model and scaler"""
        self.model.save(f'{path}/gesture_model.h5')
        pickle.dump(self.scaler, open(f'{path}/scaler.pkl', 'wb'))

    def load(self, path: str):
        """Load saved model"""
        self.model = tf.keras.models.load_model(f'{path}/gesture_model.h5')
        self.scaler = pickle.load(open(f'{path}/scaler.pkl', 'rb'))
```

---

## Real-Time Processing Architecture

### 1. Multi-Threaded Application Design

```python
"""
Skill: Design thread-safe, real-time processing applications

Key Principles:
- Separate UI thread from processing threads
- Use thread-safe queues for communication
- Minimize lock contention
- Handle graceful shutdown
"""

from threading import Thread, Event, Lock
from queue import Queue, Empty
import time

class RealTimeGestureApp:
    """Multi-threaded gesture recognition application"""

    def __init__(self):
        # Synchronization primitives
        self.stop_event = Event()
        self.pause_event = Event()

        # Thread-safe queues
        self.frame_queue = Queue(maxsize=1)      # Latest frame
        self.gesture_queue = Queue()              # Recognized gestures
        self.action_queue = Queue()               # Actions to execute

        # State
        self.lock = Lock()
        self.fps_counter = {'frames': 0, 'start_time': time.time()}

        # Components
        self.camera = None
        self.gesture_classifier = None
        self.action_executor = None

    def vision_thread_worker(self):
        """Dedicated thread for camera + gesture processing"""

        print("üé• Vision thread started")
        frame_count = 0

        while not self.stop_event.is_set():
            if self.pause_event.is_set():
                time.sleep(0.05)
                continue

            try:
                # Read frame (blocking, but that's OK)
                ret, frame = self.camera.read()
                if not ret:
                    continue

                frame_count += 1

                # Skip frames if queue full (don't buffer)
                if not self.frame_queue.empty():
                    try:
                        self.frame_queue.get_nowait()
                    except Empty:
                        pass

                # Process frame
                start = time.perf_counter()

                landmarks = self.detect_hands(frame)
                gesture = self.gesture_classifier.classify(landmarks)

                elapsed_ms = (time.perf_counter() - start) * 1000

                # Put results in queue
                try:
                    self.frame_queue.put_nowait((frame, landmarks, gesture, elapsed_ms))
                except:
                    pass

                # Update FPS counter
                with self.lock:
                    self.fps_counter['frames'] += 1

            except Exception as e:
                print(f"‚ö†Ô∏è  Vision thread error: {e}")
                time.sleep(0.01)

    def action_execution_thread_worker(self):
        """Dedicated thread for executing actions"""

        print("‚ö° Action execution thread started")

        while not self.stop_event.is_set():
            try:
                # Get action from queue (timeout prevents blocking)
                action = self.action_queue.get(timeout=0.1)

                if action:
                    self.action_executor.execute(action)

            except Empty:
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è  Action execution error: {e}")

    def ui_update_loop(self):
        """Main thread: UI updates (Tkinter requires this)"""

        print("üñ•Ô∏è  UI thread started")

        while not self.stop_event.is_set():
            try:
                # Get latest frame and gesture (non-blocking)
                frame, landmarks, gesture, elapsed = self.frame_queue.get_nowait()

                # Update UI
                self.update_display(frame, landmarks, gesture, elapsed)

                # Queue action if gesture detected
                if gesture:
                    action = self.gesture_to_action(gesture)
                    if action:
                        self.action_queue.put(action)

            except Empty:
                # No frame ready, yield CPU
                self.root.update()
                time.sleep(0.01)
            except Exception as e:
                print(f"‚ö†Ô∏è  UI thread error: {e}")
                self.root.update()
                time.sleep(0.01)

    def start(self):
        """Start all threads"""

        # Start vision thread
        vision_thread = Thread(target=self.vision_thread_worker, daemon=True)
        vision_thread.start()

        # Start action execution thread
        action_thread = Thread(target=self.action_execution_thread_worker, daemon=True)
        action_thread.start()

        # UI loop runs on main thread
        self.ui_update_loop()

    def stop(self):
        """Gracefully shutdown"""

        print("Shutting down...")
        self.stop_event.set()
        time.sleep(0.5)  # Let threads finish
        print("‚úÖ Shutdown complete")

    def get_fps(self) -> float:
        """Get current FPS"""

        with self.lock:
            frames = self.fps_counter['frames']
            elapsed = time.time() - self.fps_counter['start_time']

        if elapsed > 0:
            return frames / elapsed
        return 0
```

### 2. Event Bus Pattern for Loose Coupling

```python
"""
Skill: Implement event-driven architecture with event bus

Advantages:
- Decouples components (don't need direct references)
- Easier testing (can mock events)
- Easier to extend (add new listeners)
"""

from typing import Callable, Dict, List
from enum import Enum

class EventType(Enum):
    """Define application events"""
    GESTURE_DETECTED = "gesture_detected"
    HAND_DETECTED = "hand_detected"
    HAND_LOST = "hand_lost"
    GESTURE_EXECUTED = "gesture_executed"
    ERROR_OCCURRED = "error_occurred"
    SETTINGS_CHANGED = "settings_changed"

class Event:
    """Event object carrying data"""

    def __init__(self, event_type: EventType, data: dict = None):
        self.type = event_type
        self.data = data or {}
        self.timestamp = time.time()

class EventBus:
    """Singleton event bus for pub-sub communication"""

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.listeners: Dict[EventType, List[Callable]] = {}
        return cls._instance

    def subscribe(self, event_type: EventType, handler: Callable):
        """Subscribe to event"""

        if event_type not in self.listeners:
            self.listeners[event_type] = []

        self.listeners[event_type].append(handler)

    def unsubscribe(self, event_type: EventType, handler: Callable):
        """Unsubscribe from event"""

        if event_type in self.listeners:
            self.listeners[event_type].remove(handler)

    def publish(self, event: Event):
        """Publish event to all subscribers"""

        if event.type in self.listeners:
            for handler in self.listeners[event.type]:
                try:
                    handler(event)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Event handler error: {e}")

# Usage example
class GestureController:
    """Component that publishes gesture events"""

    def __init__(self):
        self.event_bus = EventBus()

    def on_gesture_detected(self, gesture_name: str, confidence: float):
        """Called when gesture recognized"""

        event = Event(
            EventType.GESTURE_DETECTED,
            data={
                'gesture': gesture_name,
                'confidence': confidence
            }
        )

        self.event_bus.publish(event)

class ShortcutExecutor:
    """Component that listens to gesture events"""

    def __init__(self, shortcut_map: Dict):
        self.event_bus = EventBus()
        self.shortcut_map = shortcut_map

        # Subscribe to gesture events
        self.event_bus.subscribe(EventType.GESTURE_DETECTED, self.on_gesture)

    def on_gesture(self, event: Event):
        """Handle gesture event"""

        gesture = event.data['gesture']

        if gesture in self.shortcut_map:
            shortcut = self.shortcut_map[gesture]
            self.execute_shortcut(shortcut)

    def execute_shortcut(self, shortcut: str):
        """Execute keyboard shortcut"""

        print(f"Executing: {shortcut}")
        # pyautogui.hotkey() etc.
```

---

## GUI Development

### 1. Modern CustomTkinter Interface

```python
"""
Skill: Building modern, professional UI with CustomTkinter

Advantages over standard Tkinter:
- Modern Windows 11 dark theme
- Better looking widgets
- Smooth animations
- Rounded corners
"""

import customtkinter as ctk
from PIL import Image, ImageTk
import numpy as np
import cv2

class ModernGestureUI:
    """Professional gesture controller UI"""

    def __init__(self):
        # Setup CustomTkinter
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")

        # Create main window
        self.root = ctk.CTk()
        self.root.title("Air Gesture Controller")
        self.root.geometry("1200x700")
        self.root.resizable(True, True)

        # Setup layout
        self._setup_layout()
        self._setup_styles()

    def _setup_layout(self):
        """Create UI layout"""

        # Main container
        main_container = ctk.CTkFrame(self.root, fg_color="transparent")
        main_container.pack(fill="both", expand=True, padx=10, pady=10)

        # Left panel: Video feed
        left_panel = ctk.CTkFrame(main_container, corner_radius=10)
        left_panel.pack(side="left", fill="both", expand=True, padx=(0, 10))

        # Video label
        self.video_label = ctk.CTkLabel(
            left_panel,
            text="Camera Feed",
            fg_color="#1a1a1a",
            text_color="gray"
        )
        self.video_label.pack(fill="both", expand=True, padx=5, pady=5)

        # Right panel: Controls
        right_panel = ctk.CTkFrame(main_container, width=300, fg_color="#1a1a1a")
        right_panel.pack(side="right", fill="both", padx=0)

        # Status cards
        self._create_status_cards(right_panel)

        # Control buttons
        self._create_control_buttons(right_panel)

        # Settings panel
        self._create_settings_panel(right_panel)

    def _create_status_cards(self, parent):
        """Create status information cards"""

        # Frame for cards
        cards_frame = ctk.CTkFrame(parent, fg_color="transparent")
        cards_frame.pack(fill="x", padx=10, pady=10)

        # FPS Card
        self.fps_card = ctk.CTkFrame(cards_frame, corner_radius=10, fg_color="#2a2a2a")
        self.fps_card.pack(fill="x", pady=5)

        ctk.CTkLabel(
            self.fps_card,
            text="FPS",
            text_color="gray",
            font=("Arial", 10)
        ).pack(anchor="w", padx=10, pady=(5, 0))

        self.fps_label = ctk.CTkLabel(
            self.fps_card,
            text="0 FPS",
            text_color="cyan",
            font=("Arial", 18, "bold")
        )
        self.fps_label.pack(anchor="w", padx=10, pady=(0, 5))

        # Last Gesture Card
        self.gesture_card = ctk.CTkFrame(cards_frame, corner_radius=10, fg_color="#2a2a2a")
        self.gesture_card.pack(fill="x", pady=5)

        ctk.CTkLabel(
            self.gesture_card,
            text="Last Gesture",
            text_color="gray",
            font=("Arial", 10)
        ).pack(anchor="w", padx=10, pady=(5, 0))

        self.gesture_label = ctk.CTkLabel(
            self.gesture_card,
            text="Waiting...",
            text_color="lime",
            font=("Arial", 16, "bold")
        )
        self.gesture_label.pack(anchor="w", padx=10, pady=(0, 5))

        # Hand Detection Card
        self.hand_card = ctk.CTkFrame(cards_frame, corner_radius=10, fg_color="#2a2a2a")
        self.hand_card.pack(fill="x", pady=5)

        ctk.CTkLabel(
            self.hand_card,
            text="Hands Detected",
            text_color="gray",
            font=("Arial", 10)
        ).pack(anchor="w", padx=10, pady=(5, 0))

        self.hand_label = ctk.CTkLabel(
            self.hand_card,
            text="0",
            text_color="orange",
            font=("Arial", 16, "bold")
        )
        self.hand_label.pack(anchor="w", padx=10, pady=(0, 5))

    def _create_control_buttons(self, parent):
        """Create main control buttons"""

        button_frame = ctk.CTkFrame(parent, fg_color="transparent")
        button_frame.pack(fill="x", padx=10, pady=10)

        # Start/Stop button
        self.start_button = ctk.CTkButton(
            button_frame,
            text="‚ñ∂ Start Camera",
            command=self.on_start_clicked,
            font=("Arial", 12, "bold"),
            height=40,
            corner_radius=10
        )
        self.start_button.pack(fill="x", pady=5)

        # Stop button
        self.stop_button = ctk.CTkButton(
            button_frame,
            text="‚èπ Stop Camera",
            command=self.on_stop_clicked,
            fg_color="#FF6B6B",
            hover_color="#FF5252",
            font=("Arial", 12, "bold"),
            height=40,
            corner_radius=10
        )
        self.stop_button.pack(fill="x", pady=5)

        # Settings button
        self.settings_button = ctk.CTkButton(
            button_frame,
            text="‚öôÔ∏è Settings",
            command=self.on_settings_clicked,
            font=("Arial", 12, "bold"),
            height=40,
            corner_radius=10
        )
        self.settings_button.pack(fill="x", pady=5)

    def _create_settings_panel(self, parent):
        """Create settings controls"""

        settings_frame = ctk.CTkFrame(parent, fg_color="#2a2a2a", corner_radius=10)
        settings_frame.pack(fill="x", padx=10, pady=10)

        ctk.CTkLabel(
            settings_frame,
            text="Settings",
            font=("Arial", 12, "bold")
        ).pack(anchor="w", padx=10, pady=(10, 5))

        # Sensitivity slider
        ctk.CTkLabel(
            settings_frame,
            text="Sensitivity",
            text_color="gray",
            font=("Arial", 10)
        ).pack(anchor="w", padx=10)

        self.sensitivity_slider = ctk.CTkSlider(
            settings_frame,
            from_=0.3,
            to=0.9,
            number_of_steps=6,
            command=self.on_sensitivity_changed
        )
        self.sensitivity_slider.set(0.5)
        self.sensitivity_slider.pack(fill="x", padx=10, pady=5)

        # Profile selector
        ctk.CTkLabel(
            settings_frame,
            text="Profile",
            text_color="gray",
            font=("Arial", 10)
        ).pack(anchor="w", padx=10)

        self.profile_combo = ctk.CTkComboBox(
            settings_frame,
            values=["Default", "PowerPoint", "Chrome"],
            command=self.on_profile_changed
        )
        self.profile_combo.set("Default")
        self.profile_combo.pack(fill="x", padx=10, pady=5)

    def update_display(self, frame: np.ndarray, gesture: str = None, fps: int = 0):
        """Update video display"""

        # Convert frame to PhotoImage
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_resized = cv2.resize(frame_rgb, (640, 480))
        image = Image.fromarray(frame_resized)
        photo = ImageTk.PhotoImage(image)

        # Update label
        self.video_label.configure(image=photo)
        self.video_label.image = photo

        # Update status
        if fps > 0:
            self.fps_label.configure(text=f"{fps} FPS")

        if gesture:
            self.gesture_label.configure(text=gesture)

    def on_start_clicked(self):
        """Start button callback"""
        print("Start camera")

    def on_stop_clicked(self):
        """Stop button callback"""
        print("Stop camera")

    def on_settings_clicked(self):
        """Settings button callback"""
        print("Open settings")

    def on_sensitivity_changed(self, value):
        """Sensitivity slider callback"""
        print(f"Sensitivity: {value}")

    def on_profile_changed(self, value):
        """Profile dropdown callback"""
        print(f"Profile: {value}")

    def _setup_styles(self):
        """Setup custom styles"""
        pass

    def run(self):
        """Start UI"""
        self.root.mainloop()
```

### 2. Real-Time Overlay Display

```python
"""
Skill: Create floating overlay window for presentation mode
"""

import tkinter as tk

class OverlayWindow:
    """Floating window for presentation mode"""

    def __init__(self, width: int = 320, height: int = 180):
        self.width = width
        self.height = height

        # Create transparent overlay window
        self.overlay = tk.Toplevel()
        self.overlay.geometry(f"{width}x{height}+100+100")
        self.overlay.attributes('-topmost', True)  # Always on top
        self.overlay.attributes('-alpha', 0.8)     # Semi-transparent

        # Remove window decorations
        self.overlay.overrideredirect(False)

        # Video label
        self.video_label = tk.Label(
            self.overlay,
            bg="black",
            fg="white"
        )
        self.video_label.pack(fill="both", expand=True)

    def update_frame(self, frame: np.ndarray):
        """Update overlay with new frame"""

        # Resize frame
        frame_resized = cv2.resize(frame, (self.width, self.height))
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

        # Convert to PhotoImage
        image = Image.fromarray(frame_rgb)
        photo = ImageTk.PhotoImage(image)

        # Update
        self.video_label.configure(image=photo)
        self.video_label.image = photo

    def hide(self):
        """Hide overlay"""
        self.overlay.withdraw()

    def show(self):
        """Show overlay"""
        self.overlay.deiconify()

    def close(self):
        """Close overlay"""
        self.overlay.destroy()
```

---

## System Integration

### 1. Keyboard & Mouse Control

```python
"""
Skill: Execute keyboard shortcuts and mouse control

Key Module: pyautogui
"""

import pyautogui
import time

class ShortcutExecutor:
    """Execute keyboard shortcuts based on gestures"""

    # Safety: Add small delay between actions
    ACTION_DELAY = 0.1  # 100ms

    def __init__(self):
        # Gesture to shortcut mapping
        self.shortcut_map = {
            'SWIPE_LEFT': ('left',),           # Left arrow
            'SWIPE_RIGHT': ('right',),         # Right arrow
            'THUMBS_UP': ('up',),              # Up arrow
            'THUMBS_DOWN': ('down',),          # Down arrow
            'PEACE_SIGN': ('ctrl', 'z'),       # Undo
            'OK_SIGN': ('return',),            # Enter
        }

        self.last_action_time = 0

    def execute(self, gesture: str):
        """Execute shortcut for gesture"""

        # Prevent double-triggering (debounce)
        now = time.time()
        if now - self.last_action_time < self.ACTION_DELAY:
            return

        if gesture in self.shortcut_map:
            shortcut = self.shortcut_map[gesture]
            self._execute_hotkey(*shortcut)
            self.last_action_time = now

    def _execute_hotkey(self, *keys):
        """Execute keyboard hotkey"""

        try:
            pyautogui.hotkey(*keys)
        except Exception as e:
            print(f"‚ùå Hotkey error: {e}")

    def type_text(self, text: str):
        """Type text"""

        time.sleep(0.1)
        pyautogui.typewrite(text, interval=0.05)

    def move_mouse(self, x: int, y: int):
        """Move mouse to position"""

        pyautogui.moveTo(x, y, duration=0.2)

    def click(self, button: str = 'left'):
        """Click mouse button"""

        pyautogui.click(button=button)
```

### 2. Windows API Integration

```python
"""
Skill: Interact with Windows system using pywin32

Features:
- Get active window
- Get window title
- Switch windows
- Get screen resolution
"""

import win32gui
import win32process
import win32api
import win32con
from ctypes import windll, c_long

class WindowManager:
    """Manage Windows and integrate with system"""

    def get_active_window_title(self) -> str:
        """Get title of currently active window"""

        hwnd = win32gui.GetForegroundWindow()
        title = win32gui.GetWindowText(hwnd)

        return title

    def get_active_window_class(self) -> str:
        """Get class name of active window"""

        hwnd = win32gui.GetForegroundWindow()
        class_name = win32gui.GetClassName(hwnd)

        return class_name

    def get_active_window_process(self) -> str:
        """Get process name (executable) of active window"""

        hwnd = win32gui.GetForegroundWindow()
        _, pid = win32process.GetWindowThreadProcessId(hwnd)

        try:
            handle = windll.kernel32.OpenProcess(1, False, pid)
            name = windll.psapi.GetModuleFileNameExW(handle, None, 260)
            # Extract filename from path
            return name.split('\\')[-1]
        except:
            return "Unknown"

    def get_mouse_position(self) -> tuple:
        """Get current mouse position"""

        return win32api.GetCursorPos()

    def get_screen_resolution(self) -> tuple:
        """Get screen width and height"""

        width = win32api.GetSystemMetrics(0)
        height = win32api.GetSystemMetrics(1)

        return width, height

    def set_window_position(self, hwnd: int, x: int, y: int, width: int, height: int):
        """Move and resize window"""

        win32gui.MoveWindow(hwnd, x, y, width, height, True)

    def is_presentation_mode(self) -> bool:
        """Detect if PowerPoint presentation is running"""

        title = self.get_active_window_title()
        process = self.get_active_window_process()

        return 'POWERPNT.EXE' in process.upper() or 'Presentation' in title

    def detect_fullscreen(self) -> bool:
        """Detect if full-screen application is active"""

        hwnd = win32gui.GetForegroundWindow()
        rect = win32gui.GetWindowRect(hwnd)

        width, height = self.get_screen_resolution()

        # Check if window covers entire screen
        return (rect[0] == 0 and rect[1] == 0 and
                rect[2] == width and rect[3] == height)
```

---

## Advanced Optimization Techniques

### 1. Performance Profiling

```python
"""
Skill: Measure and optimize performance

Key Metrics:
- FPS (frames per second)
- Latency (time from gesture to action)
- CPU usage
- Memory usage
"""

import time
from collections import deque

class PerformanceProfiler:
    """Profile application performance"""

    def __init__(self, window_size: int = 30):
        self.frame_times = deque(maxlen=window_size)
        self.detection_times = deque(maxlen=window_size)
        self.action_times = deque(maxlen=window_size)

    def record_frame_time(self, elapsed_ms: float):
        """Record time to capture and process frame"""
        self.frame_times.append(elapsed_ms)

    def record_detection_time(self, elapsed_ms: float):
        """Record gesture detection time"""
        self.detection_times.append(elapsed_ms)

    def record_action_time(self, elapsed_ms: float):
        """Record action execution time"""
        self.action_times.append(elapsed_ms)

    def get_fps(self) -> float:
        """Calculate current FPS"""

        if not self.frame_times:
            return 0

        avg_frame_time = sum(self.frame_times) / len(self.frame_times)
        if avg_frame_time == 0:
            return 0

        return 1000 / avg_frame_time

    def get_avg_latency(self) -> float:
        """Get average total latency"""

        all_times = (
            list(self.frame_times) +
            list(self.detection_times) +
            list(self.action_times)
        )

        if not all_times:
            return 0

        return sum(all_times) / len(all_times)

    def get_stats(self) -> dict:
        """Get comprehensive stats"""

        def stats(times):
            if not times:
                return {'avg': 0, 'min': 0, 'max': 0}
            return {
                'avg': sum(times) / len(times),
                'min': min(times),
                'max': max(times)
            }

        return {
            'fps': self.get_fps(),
            'frame_times': stats(self.frame_times),
            'detection_times': stats(self.detection_times),
            'action_times': stats(self.action_times),
            'total_latency': self.get_avg_latency()
        }

    def print_stats(self):
        """Print formatted stats"""

        stats = self.get_stats()
        print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë   Performance Statistics          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚îÇ FPS: {stats['fps']:.1f}
‚îÇ Frame Time: {stats['frame_times']['avg']:.1f}ms (max: {stats['frame_times']['max']:.1f}ms)
‚îÇ Detection: {stats['detection_times']['avg']:.1f}ms (max: {stats['detection_times']['max']:.1f}ms)
‚îÇ Action Exec: {stats['action_times']['avg']:.1f}ms (max: {stats['action_times']['max']:.1f}ms)
‚îÇ Total Latency: {stats['total_latency']:.1f}ms
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """)

# Usage
@measure_performance
def process_gesture(landmarks):
    """Automatically measures execution time"""
    # Gesture processing
    pass

def measure_performance(func):
    """Decorator to measure function time"""
    import functools

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter() - start) * 1000
        print(f"{func.__name__}: {elapsed:.1f}ms")
        return result

    return wrapper
```

### 2. Adaptive Frame Skipping

```python
"""
Skill: Adapt processing based on CPU load

When FPS drops, skip frames to maintain responsiveness
"""

class AdaptiveFrameProcessor:
    """Intelligently skip frames based on system load"""

    def __init__(self, target_fps: int = 30):
        self.target_fps = target_fps
        self.frame_skip = 0
        self.processing_times = deque(maxlen=10)
        self.frame_count = 0

    def should_process(self) -> bool:
        """Decide if current frame should be processed"""

        self.frame_count += 1

        # Skip frames if needed
        if self.frame_skip > 0 and self.frame_count % (self.frame_skip + 1) != 0:
            return False

        return True

    def update_performance(self, processing_time_ms: float):
        """Update skip rate based on actual performance"""

        self.processing_times.append(processing_time_ms)

        if len(self.processing_times) < 5:
            return

        avg_time = sum(self.processing_times) / len(self.processing_times)
        target_time = 1000 / self.target_fps  # ms per frame

        # Increase frame skip if too slow
        if avg_time > target_time * 2:
            self.frame_skip = min(self.frame_skip + 1, 4)
            print(f"‚ö†Ô∏è  Too slow, skipping {self.frame_skip} frames")

        # Decrease frame skip if fast enough
        elif avg_time < target_time and self.frame_skip > 0:
            self.frame_skip -= 1
            print(f"‚úÖ Performance improved, skipping {self.frame_skip} frames")

# Usage
processor = AdaptiveFrameProcessor(target_fps=30)

while True:
    ret, frame = camera.read()

    if processor.should_process():
        start = time.time()

        # Heavy processing
        gesture = recognizer.recognize(frame)

        elapsed = (time.time() - start) * 1000
        processor.update_performance(elapsed)
```

---

## Testing & Debugging

### 1. Unit Testing Gesture Recognition

```python
"""
Skill: Test gesture recognition thoroughly
"""

import unittest
import numpy as np

class TestGestureRecognition(unittest.TestCase):
    """Test cases for gesture classifier"""

    def setUp(self):
        """Set up test fixtures"""
        self.recognizer = StaticGestureRecognizer()

        # Generate test landmarks
        self.test_landmarks = self._create_test_landmarks()

    def _create_test_landmarks(self):
        """Create synthetic hand landmarks"""

        # 21 points, 3 coordinates each
        return np.random.rand(21, 3).tolist()

    def test_thumbs_up_recognition(self):
        """Test thumbs up detection"""

        # Create thumbs-up landmarks
        landmarks = np.zeros((21, 3)).tolist()

        # Set thumb pointing up
        landmarks[4] = [0.5, 0.2, 0.0]  # Thumb tip (above wrist)
        landmarks[0] = [0.5, 0.8, 0.0]  # Wrist (below thumb)

        # Fold other fingers
        landmarks[8] = [0.3, 0.7, 0.0]  # Index down
        landmarks[12] = [0.5, 0.7, 0.0]  # Middle down

        gesture = self.recognizer.recognize(landmarks)
        self.assertEqual(gesture, "THUMBS_UP")

    def test_ok_sign_recognition(self):
        """Test OK sign detection"""

        landmarks = np.zeros((21, 3)).tolist()

        # Thumb and index touching
        landmarks[4] = [0.5, 0.5, 0.0]  # Thumb
        landmarks[8] = [0.51, 0.51, 0.0]  # Index (very close)

        # Other fingers extended
        landmarks[12] = [0.5, 0.2, 0.0]  # Middle
        landmarks[16] = [0.6, 0.2, 0.0]  # Ring
        landmarks[20] = [0.7, 0.2, 0.0]  # Pinky

        gesture = self.recognizer.recognize(landmarks)
        self.assertEqual(gesture, "OK_SIGN")

    def test_open_palm_recognition(self):
        """Test open palm detection"""

        landmarks = np.zeros((21, 3)).tolist()

        # All fingers extended and spread
        landmarks[4] = [0.2, 0.2, 0.0]   # Thumb
        landmarks[8] = [0.8, 0.2, 0.0]   # Index
        landmarks[12] = [0.5, 0.1, 0.0]  # Middle
        landmarks[16] = [0.7, 0.3, 0.0]  # Ring
        landmarks[20] = [0.3, 0.3, 0.0]  # Pinky

        gesture = self.recognizer.recognize(landmarks)
        self.assertEqual(gesture, "OPEN_PALM")

    def test_unknown_pose(self):
        """Test unknown pose returns None"""

        landmarks = np.zeros((21, 3)).tolist()  # Random position
        gesture = self.recognizer.recognize(landmarks)
        self.assertIsNone(gesture)

    def test_performance(self):
        """Test recognition speed"""

        landmarks = self.test_landmarks

        start = time.time()
        for _ in range(1000):
            self.recognizer.recognize(landmarks)
        elapsed = (time.time() - start) * 1000

        # Should be < 50ms for 1000 iterations
        self.assertLess(elapsed, 50)

if __name__ == '__main__':
    unittest.main()
```

### 2. Integration Testing

```python
"""
Skill: Test full application workflow
"""

class TestGestureApp(unittest.TestCase):
    """Integration tests for gesture application"""

    def setUp(self):
        """Set up test app instance"""
        self.app = GestureApp()
        self.app.mock_mode = True  # Use mock camera

    def test_app_initialization(self):
        """Test app starts correctly"""

        self.assertIsNotNone(self.app.gesture_classifier)
        self.assertIsNotNone(self.app.shortcut_executor)

    def test_gesture_to_shortcut_execution(self):
        """Test gesture triggers correct shortcut"""

        # Mock shortcut execution
        executed_shortcuts = []
        original_execute = self.app.shortcut_executor.execute
        self.app.shortcut_executor.execute = lambda s: executed_shortcuts.append(s)

        # Simulate thumbs up gesture
        self.app._handle_gesture("THUMBS_UP")

        # Check shortcut was executed
        self.assertTrue(len(executed_shortcuts) > 0)

        # Restore
        self.app.shortcut_executor.execute = original_execute

    def test_profile_switching(self):
        """Test profile switching based on active window"""

        self.app.window_manager.get_active_window_process = lambda: "POWERPNT.EXE"
        self.app._update_profile()

        self.assertEqual(self.app.current_profile, "POWERPOINT")
```

---

## Deployment & Distribution

### 1. Creating Standalone Executable

*See EXE_BUILD_PLAN.md for complete guide*

### 2. Creating Installation Package

```python
"""
Skill: Package application for distribution

Step 1: Build exe with PyInstaller
Step 2: Create installer with Inno Setup
Step 3: Distribute to users
"""

# setup.py for source distribution
from setuptools import setup, find_packages

setup(
    name='AirGestureController',
    version='1.0.0',
    description='Control your computer with hand gestures',
    author='Your Name',
    packages=find_packages(),
    install_requires=[
        'opencv-python==4.8.1.78',
        'mediapipe==0.10.3',
        'pyautogui==0.9.53',
        'customtkinter==5.2.0',
        'numpy==1.24.3',
        'Pillow==10.0.0',
    ],
    entry_points={
        'console_scripts': [
            'air-gesture=src.main:main',
        ],
    },
)
```

---

## Summary: Essential Skills Checklist

### Python Fundamentals
- ‚úÖ Object-oriented design & SOLID principles
- ‚úÖ Type hints and annotations
- ‚úÖ Decorators and context managers
- ‚úÖ Exception handling
- ‚úÖ Threading and concurrency
- ‚úÖ Profiling and optimization

### Computer Vision
- ‚úÖ Image processing (filtering, transforms)
- ‚úÖ Object detection and contours
- ‚úÖ Hand landmark detection (MediaPipe)
- ‚úÖ Camera calibration
- ‚úÖ Real-time video processing

### Gesture Recognition
- ‚úÖ Static gesture classification (geometric)
- ‚úÖ Dynamic gesture recognition (swipes)
- ‚úÖ Temporal smoothing and filtering
- ‚úÖ Gesture voting and confidence
- ‚úÖ ML-based recognition (optional)

### System Integration
- ‚úÖ Keyboard shortcut execution
- ‚úÖ Mouse control
- ‚úÖ Windows API integration
- ‚úÖ Active window detection
- ‚úÖ Full-screen detection

### GUI Development
- ‚úÖ CustomTkinter modern UI
- ‚úÖ Real-time video display
- ‚úÖ Status indicators
- ‚úÖ Settings controls
- ‚úÖ Overlay windows

### Application Architecture
- ‚úÖ Multi-threaded design
- ‚úÖ Event-driven programming
- ‚úÖ Performance profiling
- ‚úÖ Error handling
- ‚úÖ Logging and debugging

### Deployment
- ‚úÖ PyInstaller configuration
- ‚úÖ Installer creation
- ‚úÖ Windows packaging
- ‚úÖ Version management

---

## Conclusion

Building a professional air gesture control application requires mastery across multiple domains:

1. **Computer Vision:** Understand image processing and hand detection
2. **Real-Time Processing:** Design efficient, multi-threaded systems
3. **Gesture Recognition:** Both geometric and ML-based approaches
4. **System Integration:** Control OS and applications
5. **Professional UI:** Create modern, responsive interfaces
6. **Performance:** Optimize for smooth, low-latency operation

This guide provides the foundation for building state-of-the-art gesture control applications. Each skill builds on previous ones, starting from Python fundamentals and advancing to production-ready deployment.

### Next Steps

1. **Learn OpenCV:** Complete tutorials on image processing
2. **Master MediaPipe:** Practice hand detection on various scenarios
3. **Build Components:** Create and test gesture recognizers
4. **Integrate:** Combine into unified application
5. **Optimize:** Profile and improve performance
6. **Deploy:** Package and distribute to users

---

**Happy Coding! üéØ**
